{
  "metadata": {
    "kernelspec": {
      "display_name": "Pyolite",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "H&M RECOMMENDATION SYSTEM USING THE FACTORIZTION METHOD",
      "metadata": {},
      "id": "a35eeb9f-df70-4ab1-a243-2d2025888eb0"
    },
    {
      "cell_type": "markdown",
      "source": "IMPORT PACKAGES",
      "metadata": {},
      "id": "11ecc03b-0158-4500-90e6-b64af29bc21d"
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport datetime\nfrom tqdm import tqdm",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "edc36e62-85f5-416b-9d85-8c2843fbe321"
    },
    {
      "cell_type": "markdown",
      "source": "THE MODEL CLASS",
      "metadata": {},
      "id": "b512759c-42d1-4ee5-9330-7a4cde06f54b"
    },
    {
      "cell_type": "code",
      "source": "# Create a class for recommendation system\nclass Recommend:\n    def __init__(self, transactions, customer_id2index, customer_index2id, article_id2index, article_index2id,\n                 default_recommendation, num_components=20):\n\n        self.transactions = transactions  # Save the transactions\n        self.negative_transactions = self.transactions.copy()  # Generate negative samples\n        self.default_recommendation = default_recommendation  # Save the default recommendation\n        self.customer_id2index = customer_id2index  # Save customer and article mapping functions\n        self.customer_index2id = customer_index2id\n        self.article_id2index = article_id2index\n        self.article_index2id = article_index2id\n        self.max_feature_values = self.transactions.max().values  # Compute the length of the one-hot features\n        self.dimensionality = self.max_feature_values.sum() + len(self.max_feature_values)\n        self.global_bias = np.random.normal(scale=1, size=1)  # Initialize the biases and parameters\n        self.biases = np.random.normal(scale=1 / self.dimensionality, size=self.dimensionality)\n        self.params = np.random.normal(scale=1. / self.dimensionality, size=(num_components, self.dimensionality))\n        self.training_indices = np.arange(len(self.transactions) * 2)  # Create the list of indices\n\n    # Stochastic gradient descent\n    def __sgd__(self, lr, reg_w, reg_v):\n        for idx in self.training_indices:\n            # Get the sample from transactions\n            if idx < len(self.transactions):\n                sample = self.transactions.iloc[idx]\n                bought = 1\n            else:\n                sample = self.negative_transactions.iloc[idx - len(self.transactions)]\n                bought = 0\n\n            # Get the encoding positions for the current sample\n            sample_positions = []\n            accum = 0\n            for idx, col in enumerate(sample.index):\n                sample_positions.append(sample[col])\n                accum += self.max_feature_values[idx]\n            sample_positions = np.array(sample_positions)\n\n            # Make a prediction\n            prediction, summed = self.__predict__(sample_positions)\n\n            # Compute the error\n            self.__log_loss__(prediction, bought)\n\n            # Compute the gradient error\n            error_gradient = -bought / (np.exp(bought * prediction) + 1.0)\n\n            # Update biases and parameters\n            self.global_bias -= lr * error_gradient\n\n            self.biases[sample_positions] -= lr * (error_gradient + 2 * reg_w * self.biases[sample_positions])\n\n            self.params[:, sample_positions] -= lr * (error_gradient * (\n                    summed[:, np.newaxis] * self.params[:, sample_positions]) + 2 * reg_v * self.params[:,\n                                                                                            sample_positions])\n\n    # Make a prediction\n    def __predict__(self, sample_positions):\n        # Compute the sum of the square component\n        summed = np.sum(self.params[:, sample_positions], axis=1)\n        summed_square = np.sum(self.params[:, sample_positions] ** 2, axis=1)\n\n        # Return the prediction using the biases and parameters\n        return self.global_bias + np.sum(self.biases[sample_positions]) + 0.5 * np.sum(\n            summed ** 2 - summed_square), summed\n\n    # Log loss error\n    def __log_loss__(self, pred, real):\n        return np.log(np.exp(-pred * real) + 1.0)\n\n    # Train the model\n    def fit(self, n_epochs=10, learning_rate=0.001, reg_w=0.01, reg_v=0.001):\n        for epoch in range(n_epochs):\n            print('Epoch:', epoch)\n            # Shuffle negative sample articles\n            self.__shuffle_negative_transactions__()\n\n            # Shuffle the training indices\n            np.random.shuffle(self.training_indices)\n\n            # Run the SGD\n            self.__sgd__(learning_rate, reg_w, reg_v)\n\n    # Shuffle negative samples\n    def __shuffle_negative_transactions__(self):\n        self.negative_transactions['article_index'] = self.negative_transactions['article_index'].sample(frac=1)\n\n    # Predict the articles for each user\n    def predict(self, customers, last_bought_articles):\n        recommendations = []\n\n        # Create the articles matrix\n        len_articles = len(self.article_index2id)\n        articles = np.eye(len_articles)\n\n        # Compute the matrix product between articles and the bias vector that apply to articles\n        len_customers = len(self.customer_index2id)\n        article_bias = np.dot(articles, self.biases[len_customers:len_customers + len_articles])\n\n        # Compute the matrix product between articles and the vectors from params that apply to articles\n        article_params = np.dot(articles, self.params[:, len_customers:len_customers + len_articles].T)\n\n        # Compute the matrix product between articles and the vectors from params to the square that apply to articles\n        article_square_params = np.dot(articles, self.params[:, len_customers:len_customers + len_articles].T ** 2)\n\n        for customer, last_bought_article in zip(customers, last_bought_articles):\n            # If the customer is not in the trained ones return the default recommendation\n            if customer not in self.customer_id2index.keys():\n                recommendations.append(' '.join(default_recommendation))\n\n            # Else use the factorization machine\n            else:\n                customer_idx = self.customer_id2index[customer]\n                last_bought_idx = self.article_id2index[last_bought_article] + len_customers + len_articles\n\n                # Make a prediction for each article using the one hot matrix\n                bias_product = self.biases[customer_idx] + article_bias + self.biases[last_bought_idx]\n                params_product = self.params[:, customer_idx] + article_params + self.params[:, last_bought_idx]\n                params_product_square = self.params[:, customer_idx] ** 2 + article_square_params + self.params[:,\n                                                                                                last_bought_idx] ** 2\n\n                predictions = self.global_bias + bias_product + 0.5 * np.sum(\n                    params_product ** 2 - params_product_square, axis=1)\n\n                # Sort the predictions and keep the 12 higher\n                recommended_indexes = predictions.argsort()[-12:]\n\n                # Keep the recommendations for this customer\n                recommendations.append(' '.join([self.article_index2id[item_idx] for item_idx in recommended_indexes]))\n\n        return pd.DataFrame({\n            'customer_id': customers,\n            'article_id recommendations': recommendations,\n        })",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [],
      "id": "360af8d7-6caa-49fa-97ac-1650973cc55e"
    },
    {
      "cell_type": "markdown",
      "source": "LOAD TRANSACTIONS DATA",
      "metadata": {},
      "id": "9b9aea1c-de46-48e6-95ba-7c45ca412cd8"
    },
    {
      "cell_type": "code",
      "source": "# Load data set\ntransactions = pd.read_csv(\"transactions_train.csv\", dtype={'article_id': str})\ntransactions.drop(['sales_channel_id', 'price'], inplace=True, axis=1)\n",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "899c2678-d5ea-4f9b-87dc-47cf2d064a79"
    },
    {
      "cell_type": "code",
      "source": "# Filter transactions by date (approximately one week start-end date)\nstart_date = datetime.datetime(2020, 9, 14)\ntransactions[\"t_dat\"] = pd.to_datetime(transactions[\"t_dat\"])\ntransactions = transactions.loc[transactions[\"t_dat\"] >= start_date]",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "0c037d23-46d5-4a00-8043-23630576944a"
    },
    {
      "cell_type": "code",
      "source": "# Filter transactions by number of an article has been bought\narticle_bought_count = transactions[['article_id', 't_dat']].groupby('article_id').count().reset_index().rename(\n    columns={'t_dat': 'count'})\nmost_bought_articles = article_bought_count[article_bought_count['count'] > 10]['article_id'].values\ntransactions = transactions[transactions['article_id'].isin(most_bought_articles)]\ntransactions = transactions.reset_index(drop=True)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "3e4beff2-ed49-4b16-be3b-daa31d1754a7"
    },
    {
      "cell_type": "markdown",
      "source": "LOAD CUSTOMERS DATA",
      "metadata": {},
      "id": "582c5add-c085-42b5-9308-39f1ac22293a"
    },
    {
      "cell_type": "code",
      "source": "# Make the predictions for 1000 customers\ncustomers = pd.read_csv(\"sample_submission.csv\", nrows=1000).customer_id.values\n\n# Last bought articles for each customer\nlast_bought_articles = transactions.sort_values(['customer_id', 't_dat'], ascending=False).drop_duplicates(\n    ['customer_id'], keep='first')\n",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "afcf105e-f702-49bd-9f55-5a668d823ce5"
    },
    {
      "cell_type": "code",
      "source": "# Create an array with the last bought article for each customer\ndef get_last_bought_article_per_customer(customers, last_bought_articles):\n    last_articles = []\n    transaction_customers = last_bought_articles.customer_id.values\n    for customer in tqdm(customers):\n        if customer in transaction_customers:\n            last_articles.append(\n                last_bought_articles[last_bought_articles['customer_id'] == customer].article_id.values[0])\n        else:\n            last_articles.append(None)\n    return np.array(last_articles)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "ef09cbc5-481e-44ad-8ce4-f8a29f5073e7"
    },
    {
      "cell_type": "code",
      "source": "# Get the last bought articles\nlast_articles = get_last_bought_article_per_customer(customers, last_bought_articles)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "93b5365e-1c0c-495c-93ad-a4c679e6759d"
    },
    {
      "cell_type": "markdown",
      "source": "CALCULATE DEFAULT RECOMMENDATION",
      "metadata": {},
      "id": "ac16e11d-371d-47f4-8502-2998644636f9"
    },
    {
      "cell_type": "code",
      "source": "# Calculate time decaying popularity\ntransactions['pop_factor'] = transactions['t_dat'].apply(lambda x: 1 / (datetime.datetime(2020, 9, 23) - x).days)\ntransactions_by_article = transactions[['article_id', 'pop_factor']].groupby('article_id').sum().reset_index()\ndefault_recommendation = transactions_by_article.sort_values(by='pop_factor', ascending=False)['article_id'].values[:12]",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "e36feeda-f4e7-49c8-94f3-afba216899ce"
    },
    {
      "cell_type": "markdown",
      "source": "PREPARE DATA AND FEATURES",
      "metadata": {},
      "id": "75d11a28-1784-488c-a532-3b5e5a431b13"
    },
    {
      "cell_type": "code",
      "source": "# Sort the transactions by customer and date and assign the last item bought to each transaction\ntransactions = transactions.sort_values(['customer_id', 't_dat'], axis=0)\ntransactions['last_bought_id'] = pd.concat(\n    [pd.Series([transactions['article_id'].values[-1]]), transactions['article_id']])[:-1].values\ntransactions.drop(['t_dat', 'pop_factor'], inplace=True, axis=1)\n\n# Get the users and articles\ncustomer_values = np.unique(transactions.customer_id.values)\narticle_values = np.unique(transactions.article_id.values)\n\n# Map customer and article ids to indices\ncustomer_id2index = {c: i for i, c in enumerate(customer_values)}\narticle_id2index = {a: i for i, a in enumerate(article_values)}\ncustomer_index2id = {i: c for c, i in customer_id2index.items()}\narticle_index2id = {i: a for a, i in article_id2index.items()}\n\n# Assign the customer and article indices to the transactions and drop the ids\ntransactions['customer_index'] = transactions.customer_id.map(customer_id2index)\ntransactions['article_index'] = transactions.article_id.map(article_id2index)\ntransactions['last_bought_index'] = transactions.last_bought_id.map(article_id2index)\ntransactions.drop(['customer_id', 'article_id', 'last_bought_id'], inplace=True, axis=1)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "ac7efe14-331e-4dfb-975e-b49989fa7e37"
    },
    {
      "cell_type": "markdown",
      "source": "TRAIN THE MODEL",
      "metadata": {},
      "id": "5e31faf4-1ada-42d9-a02d-01c591b8d5f7"
    },
    {
      "cell_type": "code",
      "source": "# Initiate the class for recommendation system\nrecSystem = Recommend(transactions, customer_id2index, customer_index2id, article_id2index, article_index2id,\n                      default_recommendation, num_components=20)\n\n# Train the model\nrecSystem.fit()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "053a3b0f-a797-4021-80b6-d7f67749d29f"
    },
    {
      "cell_type": "markdown",
      "source": "MAKE ITEM PREDICTIONS ",
      "metadata": {},
      "id": "59988153-3f1e-4836-8809-a4cb9ac2c14b"
    },
    {
      "cell_type": "code",
      "source": "# Make a prediction for each customer\npredictions = recSystem.predict(customers, last_articles)\npredictions.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "86e52dca-6305-4381-85e5-a255305f98b6"
    },
    {
      "cell_type": "markdown",
      "source": "EXTRACT PREDICTIONS TO A .CSV FILE",
      "metadata": {},
      "id": "6177a9a6-b682-4ad9-a093-272830aa6ace"
    },
    {
      "cell_type": "code",
      "source": "# Extract predictions to a .csv file\npredictions.to_csv('recommendation.csv', index=False)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "c04377cf-a572-4942-a129-7efb11d83a41"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "3981fa85-ed2e-4584-bad4-fc5620187dc9"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "737c0ac8-4e84-4252-88b5-9450a9dbaf61"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "5c0f4a32-36e7-4a4f-bb03-3521aeaabce9"
    }
  ]
}