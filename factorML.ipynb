{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H&M RECOMMENDATION SYSTEM USING THE FACTORIZTION METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE MODEL CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for recommendation system\n",
    "class Recommend:\n",
    "    def __init__(self, transactions, customer_id2index, customer_index2id, article_id2index, article_index2id,\n",
    "                 default_recommendation, num_components=20):\n",
    "\n",
    "        self.transactions = transactions  # Save the transactions\n",
    "        self.negative_transactions = self.transactions.copy()  # Generate negative samples\n",
    "        self.default_recommendation = default_recommendation  # Save the default recommendation\n",
    "        self.customer_id2index = customer_id2index  # Save customer and article mapping functions\n",
    "        self.customer_index2id = customer_index2id\n",
    "        self.article_id2index = article_id2index\n",
    "        self.article_index2id = article_index2id\n",
    "        self.max_feature_values = self.transactions.max().values  # Compute the length of the one-hot features\n",
    "        self.dimensionality = self.max_feature_values.sum() + len(self.max_feature_values)\n",
    "        self.global_bias = np.random.normal(scale=1, size=1)  # Initialize the biases and parameters\n",
    "        self.biases = np.random.normal(scale=1 / self.dimensionality, size=self.dimensionality)\n",
    "        self.params = np.random.normal(scale=1. / self.dimensionality, size=(num_components, self.dimensionality))\n",
    "        self.training_indices = np.arange(len(self.transactions) * 2)  # Create the list of indices\n",
    "\n",
    "    # Stochastic gradient descent\n",
    "    def __sgd__(self, lr, reg_w, reg_v):\n",
    "        for idx in self.training_indices:\n",
    "            # Get the sample from transactions\n",
    "            if idx < len(self.transactions):\n",
    "                sample = self.transactions.iloc[idx]\n",
    "                bought = 1\n",
    "            else:\n",
    "                sample = self.negative_transactions.iloc[idx - len(self.transactions)]\n",
    "                bought = 0\n",
    "\n",
    "            # Get the encoding positions for the current sample\n",
    "            sample_positions = []\n",
    "            accum = 0\n",
    "            for idx, col in enumerate(sample.index):\n",
    "                sample_positions.append(sample[col])\n",
    "                accum += self.max_feature_values[idx]\n",
    "            sample_positions = np.array(sample_positions)\n",
    "\n",
    "            # Make a prediction\n",
    "            prediction, summed = self.__predict__(sample_positions)\n",
    "\n",
    "            # Compute the error\n",
    "            self.__log_loss__(prediction, bought)\n",
    "\n",
    "            # Compute the gradient error\n",
    "            error_gradient = -bought / (np.exp(bought * prediction) + 1.0)\n",
    "\n",
    "            # Update biases and parameters\n",
    "            self.global_bias -= lr * error_gradient\n",
    "\n",
    "            self.biases[sample_positions] -= lr * (error_gradient + 2 * reg_w * self.biases[sample_positions])\n",
    "\n",
    "            self.params[:, sample_positions] -= lr * (error_gradient * (\n",
    "                    summed[:, np.newaxis] * self.params[:, sample_positions]) + 2 * reg_v * self.params[:,\n",
    "                                                                                            sample_positions])\n",
    "\n",
    "    # Make a prediction\n",
    "    def __predict__(self, sample_positions):\n",
    "        # Compute the sum of the square component\n",
    "        summed = np.sum(self.params[:, sample_positions], axis=1)\n",
    "        summed_square = np.sum(self.params[:, sample_positions] ** 2, axis=1)\n",
    "\n",
    "        # Return the prediction using the biases and parameters\n",
    "        return self.global_bias + np.sum(self.biases[sample_positions]) + 0.5 * np.sum(\n",
    "            summed ** 2 - summed_square), summed\n",
    "\n",
    "    # Log loss error\n",
    "    def __log_loss__(self, pred, real):\n",
    "        return np.log(np.exp(-pred * real) + 1.0)\n",
    "\n",
    "    # Train the model\n",
    "    def fit(self, n_epochs=10, learning_rate=0.001, reg_w=0.01, reg_v=0.001):\n",
    "        for epoch in range(n_epochs):\n",
    "            print('Epoch:', epoch)\n",
    "            # Shuffle negative sample articles\n",
    "            self.__shuffle_negative_transactions__()\n",
    "\n",
    "            # Shuffle the training indices\n",
    "            np.random.shuffle(self.training_indices)\n",
    "\n",
    "            # Run the SGD\n",
    "            self.__sgd__(learning_rate, reg_w, reg_v)\n",
    "\n",
    "    # Shuffle negative samples\n",
    "    def __shuffle_negative_transactions__(self):\n",
    "        self.negative_transactions['article_index'] = self.negative_transactions['article_index'].sample(frac=1)\n",
    "\n",
    "    # Predict the articles for each user\n",
    "    def predict(self, customers, last_bought_articles):\n",
    "        recommendations = []\n",
    "\n",
    "        # Create the articles matrix\n",
    "        len_articles = len(self.article_index2id)\n",
    "        articles = np.eye(len_articles)\n",
    "\n",
    "        # Compute the matrix product between articles and the bias vector that apply to articles\n",
    "        len_customers = len(self.customer_index2id)\n",
    "        article_bias = np.dot(articles, self.biases[len_customers:len_customers + len_articles])\n",
    "\n",
    "        # Compute the matrix product between articles and the vectors from params that apply to articles\n",
    "        article_params = np.dot(articles, self.params[:, len_customers:len_customers + len_articles].T)\n",
    "\n",
    "        # Compute the matrix product between articles and the vectors from params to the square that apply to articles\n",
    "        article_square_params = np.dot(articles, self.params[:, len_customers:len_customers + len_articles].T ** 2)\n",
    "\n",
    "        for customer, last_bought_article in zip(customers, last_bought_articles):\n",
    "            # If the customer is not in the trained ones return the default recommendation\n",
    "            if customer not in self.customer_id2index.keys():\n",
    "                recommendations.append(' '.join(default_recommendation))\n",
    "\n",
    "            # Else use the factorization machine\n",
    "            else:\n",
    "                customer_idx = self.customer_id2index[customer]\n",
    "                last_bought_idx = self.article_id2index[last_bought_article] + len_customers + len_articles\n",
    "\n",
    "                # Make a prediction for each article using the one hot matrix\n",
    "                bias_product = self.biases[customer_idx] + article_bias + self.biases[last_bought_idx]\n",
    "                params_product = self.params[:, customer_idx] + article_params + self.params[:, last_bought_idx]\n",
    "                params_product_square = self.params[:, customer_idx] ** 2 + article_square_params + self.params[:,\n",
    "                                                                                                last_bought_idx] ** 2\n",
    "\n",
    "                predictions = self.global_bias + bias_product + 0.5 * np.sum(\n",
    "                    params_product ** 2 - params_product_square, axis=1)\n",
    "\n",
    "                # Sort the predictions and keep the 12 higher\n",
    "                recommended_indexes = predictions.argsort()[-12:]\n",
    "\n",
    "                # Keep the recommendations for this customer\n",
    "                recommendations.append(' '.join([self.article_index2id[item_idx] for item_idx in recommended_indexes]))\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            'customer_id': customers,\n",
    "            'article_id recommendations': recommendations,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD TRANSACTIONS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "transactions = pd.read_csv(r\"C:\\Users\\Baski\\Desktop\\SeniorProject\\transactions_train.csv\", dtype={'article_id': str})\n",
    "transactions.drop(['sales_channel_id', 'price'], inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter transactions by date (approximately one week start-end date)\n",
    "start_date = datetime.datetime(2020, 9, 14)\n",
    "transactions[\"t_dat\"] = pd.to_datetime(transactions[\"t_dat\"])\n",
    "transactions = transactions.loc[transactions[\"t_dat\"] >= start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter transactions by number of an article has been bought\n",
    "article_bought_count = transactions[['article_id', 't_dat']].groupby('article_id').count().reset_index().rename(\n",
    "    columns={'t_dat': 'count'})\n",
    "most_bought_articles = article_bought_count[article_bought_count['count'] > 10]['article_id'].values\n",
    "transactions = transactions[transactions['article_id'].isin(most_bought_articles)]\n",
    "transactions = transactions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD CUSTOMERS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the predictions for 1000 customers\n",
    "customers = pd.read_csv(r\"C:\\Users\\Baski\\Desktop\\SeniorProject\\sample_submission.csv\", nrows=1000).customer_id.values\n",
    "\n",
    "# Last bought articles for each customer\n",
    "last_bought_articles = transactions.sort_values(['customer_id', 't_dat'], ascending=False).drop_duplicates(\n",
    "    ['customer_id'], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array with the last bought article for each customer\n",
    "def get_last_bought_article_per_customer(customers, last_bought_articles):\n",
    "    last_articles = []\n",
    "    transaction_customers = last_bought_articles.customer_id.values\n",
    "    for customer in tqdm(customers):\n",
    "        if customer in transaction_customers:\n",
    "            last_articles.append(\n",
    "                last_bought_articles[last_bought_articles['customer_id'] == customer].article_id.values[0])\n",
    "        else:\n",
    "            last_articles.append(None)\n",
    "    return np.array(last_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 604.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the last bought articles\n",
    "last_articles = get_last_bought_article_per_customer(customers, last_bought_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALCULATE DEFAULT RECOMMENDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time decaying popularity\n",
    "transactions['pop_factor'] = transactions['t_dat'].apply(lambda x: 1 / (datetime.datetime(2020, 9, 23) - x).days)\n",
    "transactions_by_article = transactions[['article_id', 'pop_factor']].groupby('article_id').sum().reset_index()\n",
    "default_recommendation = transactions_by_article.sort_values(by='pop_factor', ascending=False)['article_id'].values[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE DATA AND FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the transactions by customer and date and assign the last item bought to each transaction\n",
    "transactions = transactions.sort_values(['customer_id', 't_dat'], axis=0)\n",
    "transactions['last_bought_id'] = pd.concat(\n",
    "    [pd.Series([transactions['article_id'].values[-1]]), transactions['article_id']])[:-1].values\n",
    "transactions.drop(['t_dat', 'pop_factor'], inplace=True, axis=1)\n",
    "\n",
    "# Get the users and articles\n",
    "customer_values = np.unique(transactions.customer_id.values)\n",
    "article_values = np.unique(transactions.article_id.values)\n",
    "\n",
    "# Map customer and article ids to indices\n",
    "customer_id2index = {c: i for i, c in enumerate(customer_values)}\n",
    "article_id2index = {a: i for i, a in enumerate(article_values)}\n",
    "customer_index2id = {i: c for c, i in customer_id2index.items()}\n",
    "article_index2id = {i: a for a, i in article_id2index.items()}\n",
    "\n",
    "# Assign the customer and article indices to the transactions and drop the ids\n",
    "transactions['customer_index'] = transactions.customer_id.map(customer_id2index)\n",
    "transactions['article_index'] = transactions.article_id.map(article_id2index)\n",
    "transactions['last_bought_index'] = transactions.last_bought_id.map(article_id2index)\n",
    "transactions.drop(['customer_id', 'article_id', 'last_bought_id'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n"
     ]
    }
   ],
   "source": [
    "# Initiate the class for recommendation system\n",
    "recSystem = Recommend(transactions, customer_id2index, customer_index2id, article_id2index, article_index2id,\n",
    "                      default_recommendation, num_components=20)\n",
    "\n",
    "# Train the model\n",
    "recSystem.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE ITEM PREDICTIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id recommendations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>0924243002 0924243001 0918522001 0751471001 08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>0924243002 0924243001 0918522001 0751471001 08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0888229007 0923569002 0809320001 0214844002 06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
       "      <td>0924243002 0924243001 0918522001 0751471001 08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
       "      <td>0924243002 0924243001 0918522001 0751471001 08...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n",
       "3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...   \n",
       "4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...   \n",
       "\n",
       "                          article_id recommendations  \n",
       "0  0924243002 0924243001 0918522001 0751471001 08...  \n",
       "1  0924243002 0924243001 0918522001 0751471001 08...  \n",
       "2  0888229007 0923569002 0809320001 0214844002 06...  \n",
       "3  0924243002 0924243001 0918522001 0751471001 08...  \n",
       "4  0924243002 0924243001 0918522001 0751471001 08...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a prediction for each customer\n",
    "predictions = recSystem.predict(customers, last_articles)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACT PREDICTIONS TO A .CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions to a .csv file\n",
    "predictions.to_csv('recommendation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
